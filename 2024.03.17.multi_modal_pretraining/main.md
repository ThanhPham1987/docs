![thumbnail](thumbnail.png)

# Multi-Modal Pre-training (Apple's MM1)

### Links

**YouTube:** https://youtube.com/live/viiB3JmK21M

**X:** https://twitter.com/i/broadcasts/1dRKZEXgmLaxB

**Twitch:** 

**Substack:**

**ResearchHub:**

**TikTok:**

**Reddit:**

### References

MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training
https://arxiv.org/pdf/2403.09611.pdf

Simple and Scalable Strategies to Continually Pre-train Large Language Models
https://arxiv.org/pdf/2403.08763.pdf

https://arxiv.org/abs/2312.06742

## Notes

notes

### Blog

notes

### Vertical Video

title
description
hashtags
