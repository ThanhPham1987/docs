![thumbnail](thumbnail.png)

# Mixture-of-Depths

### Links

**YouTube:** https://youtube.com/live/Teru_qIdB8Y

**X:** https://twitter.com/i/broadcasts/1LyxBnEBkQkxN

**Twitch:**

**Substack:**

**ResearchHub:**

**TikTok:**

**Reddit:**

### References

Mixture-of-Depths: Dynamically allocating compute in transformer-based language models
https://arxiv.org/pdf/2404.02258.pdf

https://twitter.com/theseamouse/status/1775782800362242157?s=12&t=BSlMYtugAr8LLWXLnMLLrQ

LLM Visualization
https://bbycroft.net/llm

Deep Residual Learning for Image Recognition
https://arxiv.org/pdf/1512.03385.pdf


https://www.researchgate.net/publication/340457442/figure/fig4/AS:877300765712391@1586176210735/Comparison-between-a-a-static-computation-graph-in-TensorFlow-115-and-b-an.png
https://huyenchip.com/assets/pics/compilers/13_graph_optimization.png
https://huyenchip.com/assets/pics/compilers/8_irs.png
